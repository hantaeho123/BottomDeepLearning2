{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751cbb36-1e48-4dd7-a457-86c46d8c9a73",
   "metadata": {},
   "source": [
    "# 4. word2vec 속도 개선 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b34608-daa0-4324-ba29-d171e57b3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14d1c8-5045-4b5b-81f4-2f3b15aa27ea",
   "metadata": {},
   "source": [
    "## 4.1.2 Embedding 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851d21ab-fc9c-4337-be0c-1a4bd6742d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17],\n",
       "       [18, 19, 20]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W_in이라 가정\n",
    "W = np.arange(21).reshape(7,3)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba325f1-64ac-40e6-9cb6-055a1d39b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2번째 단어의 분산 표현:  [6 7 8]\n",
      "3번째 단어의 분산 표현:  [ 9 10 11]\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "j=3\n",
    "print(f\"{i}번째 단어의 분산 표현:  {W[i]}\")\n",
    "print(f\"{j}번째 단어의 분산 표현:  {W[j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e46ba19d-e04f-49de-b5fa-7189babfd5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  4,  5],\n",
       "       [ 0,  1,  2],\n",
       "       [18, 19, 20],\n",
       "       [ 0,  1,  2],\n",
       "       [15, 16, 17]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 분산 표현 가져오기\n",
    "idx = np.array([1,0,6,0,5])\n",
    "W[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb7eb6e-a32c-49c4-8bb1-dd6c166a45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding 계층 구현\n",
    "class Embedding :\n",
    "    def __init__(self, W) :\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx) :\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout) :\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3086804f-5202-4a99-93de-d0b47049535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW:\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "dout:\n",
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "idx:\n",
      "[0, 2, 1, 2]\n",
      "embedding층 이후\n",
      "[[ 0.  1.  2.]\n",
      " [ 6.  7.  8.]\n",
      " [12. 14. 16.]]\n"
     ]
    }
   ],
   "source": [
    "# Embeding층 값 예시\n",
    "dW = np.zeros((3,3))\n",
    "print(\"dW:\")\n",
    "print(dW)\n",
    "dout = np.arange(12).reshape(4,3)\n",
    "print(\"dout:\")\n",
    "print(dout)\n",
    "idx = [0,2,1,2]\n",
    "print(\"idx:\")\n",
    "print(idx)\n",
    "print(\"embedding층 이후\")\n",
    "np.add.at(dW,idx,dout)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d1ccd7-58ac-47d6-bef9-58689024b7cf",
   "metadata": {},
   "source": [
    "## 4.2.3 시그모이드 함수와 교차 엔트로피 차차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6588f5a6-d725-4a85-b05f-0dee855a4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidWithLoss :\n",
    "    def __init__(self) :\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = []\n",
    "        self.y = None \n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t) :\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1-self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout = 1) :\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b304a80a-503d-443a-988d-df2de0bf4742",
   "metadata": {},
   "source": [
    "## 4.2.4 다중분류에서 이진분류로(구현) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89670dd7-1902-4da7-8740-16eeda4e4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot :\n",
    "    def __init__(self, W) :\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, h, idx) :\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout) :\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3194c8-e9b3-4344-8818-939a4856ab47",
   "metadata": {},
   "source": [
    "## 4.2.6 네거티브 샘플링의  샘플링 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "904086d3-fe28-4513-9723-6d8c1edca8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 넘파이에서 샘플링하는 예\n",
    "\n",
    "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
    "np.random.choice(words)\n",
    "\n",
    "np.random.choice(words, size = 5)\n",
    "\n",
    "np.random.choice(words, size = 5, replace = False)\n",
    "\n",
    "p = [0.5,0.1,0.05,0.2,0.05,0.1]\n",
    "np.random.choice(words, p=p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4dfeae1-baec-4c56-82a3-8b199aeee2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64196878 0.33150408 0.02652714]\n"
     ]
    }
   ],
   "source": [
    "# 원래 확률이 낮은 단어의 확률을 높이기 위해 0.75 작업해주기\n",
    "p = [0.7, 0.29, 0.01]\n",
    "new_p = np.power(p, 0.75)\n",
    "new_p /= np.sum(new_p)\n",
    "print(new_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a5f7446-2f7e-4d7d-8300-9960c5ca634e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        if not GPU :\\n            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\\n\\n            for i in range(batch_size) :\\n                p = self.word_p.copy()\\n                target_idx = target[i]\\n                p[target_idx] = 0\\n                p /= p.sum()\\n                negative_sample[i, :] = np.random.choice(self.vocab_size, size = self.sample_size)\\n                \\n        else :\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *\n",
    "import collections \n",
    "# 말뭉치에서 단어의 확률분포를 만들고, 다시 0.75를 제곱한 다음 부정적 예를 샘플링 하는 클래스\n",
    "class UnigramSampler :\n",
    "    def __init__(self, corpus, power, sample_size) :\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus :\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size) :\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target) :\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = np.random.choice(self.vocab_size, size = (batch_size, self.sample_size), replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample\n",
    "    \n",
    "'''\n",
    "        if not GPU :\n",
    "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "            for i in range(batch_size) :\n",
    "                p = self.word_p.copy()\n",
    "                target_idx = target[i]\n",
    "                p[target_idx] = 0\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size = self.sample_size)\n",
    "                \n",
    "        else :\n",
    "'''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97c5fa36-5b6d-4b78-acee-e3f353333d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [4 3]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "# UnigramSampler 사용 예시\n",
    "corpus = np.array([0,1,2,3,4,1,2,3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1,3,0])\n",
    "negative_sample=sampler.get_negative_sample(target)\n",
    "print(negative_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e989f39-b2fe-49c6-b8dc-b8a0a8a52557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네거티브 샘플링 구현\n",
    "class NegativeSamplingLoss :\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size = 5) :\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers :\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, h, target) :\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "\n",
    "        # 긍정적인 예 순전파\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "\n",
    "        # 부정적인 예 순전파\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size) :\n",
    "            negative_target = negative_sample[:,i]\n",
    "            score = self.embed_dot_layers[1+i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1+i].forward(score, negative_label)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout = 1) :\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers) :\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "            \n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673c54d-a6b3-47c3-b11f-2521e8f944e7",
   "metadata": {},
   "source": [
    "## 4.3.1 CBOW 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d30f9b67-821f-468f-8bd3-cc73ceabc174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선된 CBOW 모델의 구현\n",
    "\n",
    "class CBOW :\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus) :\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        #가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size) :\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power = 0.75, sample_size=5)\n",
    "\n",
    "        # 모든 가중치와 기울기를 배열에 모은다.\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers :\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target) :\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers) :\n",
    "            h += layer.forward(contexts[:,i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout = 1) :\n",
    "        dout = self.ns_loss.backward(dout) \n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers :\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d054ee4d-1e71-4ce9-bae9-6e1dc0c3a4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e47f02-5797-4371-a5a3-6ee4fc88c521",
   "metadata": {},
   "source": [
    "## 4.3.2 CBOW 모델 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a124268-1387-4b5a-b012-991db1ad8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common import config\n",
    "#config.GPU = True\n",
    "import pickle\n",
    "from common.functions import cross_entropy_error\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eac1c711-c7f0-43f9-bef2-ad1017a03f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  27  24  39  26  40  41  42  26  43  32  44  45  46  24  47\n",
      "  26  27  28  29  48  49  41  42  50  51  52  53  54  55  35  36  37  42\n",
      "  56  57  58  59  24  35  60  42  61  62  63  64  65  66  67  68  69  70\n",
      "  35  71  72  42  73  74  75  35  46  42  76  77  64  78  79  80  27  28\n",
      "  81  82  83  24  32  61  84  26  40  85  26  62  78  86  32  26  87  88\n",
      "  89  90  64  78  91  92  93  94  95  96  97  82  98  24  26  99  32 100\n",
      "  42 101 102  26 103  93 104  66 105 106 107  26 108 109  26  67]\n",
      "of\n",
      "[[ 0  1  2  3  4  6  7  8  9 10]\n",
      " [ 1  2  3  4  5  7  8  9 10 11]\n",
      " [ 2  3  4  5  6  8  9 10 11 12]\n",
      " [ 3  4  5  6  7  9 10 11 12 13]\n",
      " [ 4  5  6  7  8 10 11 12 13 14]\n",
      " [ 5  6  7  8  9 11 12 13 14 15]\n",
      " [ 6  7  8  9 10 12 13 14 15 16]\n",
      " [ 7  8  9 10 11 13 14 15 16 17]\n",
      " [ 8  9 10 11 12 14 15 16 17 18]\n",
      " [ 9 10 11 12 13 15 16 17 18 19]]\n",
      "[ 5  6  7  8  9 10 11 12 13 14]\n",
      "----------------------------------------\n",
      "id_to_word[target[100]] :\n",
      "than\n",
      "100번째 문맥에서의 단어들\n",
      "workers\n",
      "exposed\n",
      "to\n",
      "it\n",
      "more\n",
      "N\n",
      "years\n",
      "ago\n",
      "researchers\n",
      "reported\n"
     ]
    }
   ],
   "source": [
    "# 전처리 확인기기\n",
    "print(corpus[:160])\n",
    "print(id_to_word[42])\n",
    "print(contexts[:10])\n",
    "print(target[:10])\n",
    "print(\"-\"*40)\n",
    "print(\"id_to_word[target[100]] :\")\n",
    "print(id_to_word[int(target[100])])\n",
    "print(\"100번째 문맥에서의 단어들\")\n",
    "for word_ID in map(int, contexts[100]) :\n",
    "    print(id_to_word[word_ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eba57b3d-d068-480f-b64b-637fe256b7a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, optimizer)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 학습 시작\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m trainer\u001b[38;5;241m.\u001b[39mplot()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 나중에 사용할 수 있도록 필요한 데이터 저장\u001b[39;00m\n",
      "File \u001b[1;32m~\\python_practice\\DeepLearingFromScratch2\\common\\trainer.py:40\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# 기울기 구해 매개변수 갱신\u001b[39;00m\n\u001b[0;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch_x, batch_t)\n\u001b[1;32m---> 40\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m params, grads \u001b[38;5;241m=\u001b[39m remove_duplicate(model\u001b[38;5;241m.\u001b[39mparams, model\u001b[38;5;241m.\u001b[39mgrads)  \u001b[38;5;66;03m# 공유된 가중치를 하나로 모음\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[14], line 37\u001b[0m, in \u001b[0;36mCBOW.backward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) :\n\u001b[1;32m---> 37\u001b[0m     dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mns_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     38\u001b[0m     dout \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_layers)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_layers :\n",
      "Cell \u001b[1;32mIn[13], line 36\u001b[0m, in \u001b[0;36mNegativeSamplingLoss.backward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l0, l1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dot_layers) :\n\u001b[0;32m     35\u001b[0m     dscore \u001b[38;5;241m=\u001b[39m l0\u001b[38;5;241m.\u001b[39mbackward(dout)\n\u001b[1;32m---> 36\u001b[0m     dh \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43ml1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdscore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dh\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mEmbeddingDot.backward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dout) :\n\u001b[0;32m     16\u001b[0m     h, target_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\n\u001b[1;32m---> 17\u001b[0m     dout \u001b[38;5;241m=\u001b[39m \u001b[43mdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(dout\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m     dtarget_W \u001b[38;5;241m=\u001b[39m dout \u001b[38;5;241m*\u001b[39m h\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed\u001b[38;5;241m.\u001b[39mbackward(dtarget_W)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "if config.GPU :\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "\n",
    "# 모델 등 생성\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "\n",
    "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
    "word_vecs = model.word_vecs\n",
    "if config.GPU :\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "with open(pkl_file, 'wb') as f :\n",
    "    pickle.dump(params, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfc7ec-cd45-4c2b-bff9-668d36e2f79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "349f6f04-2665-4b4c-8993-f1a12ffb9146",
   "metadata": {},
   "source": [
    "## 4.3.3 CBOW 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c55b931-995c-4539-974c-a48ba83c319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e5b4b7a-d7ff-4022-8953-229d5c4dda16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] money\n",
      " topics: 0.458740234375\n",
      " clients: 0.452880859375\n",
      " requirements: 0.423095703125\n",
      " title: 0.4052734375\n",
      " payouts: 0.39501953125\n",
      "\n",
      "[query] year\n",
      " month: 0.71875\n",
      " week: 0.65234375\n",
      " spring: 0.62744140625\n",
      " summer: 0.6259765625\n",
      " decade: 0.603515625\n",
      "\n",
      "[query] car\n",
      " luxury: 0.497314453125\n",
      " arabia: 0.47802734375\n",
      " auto: 0.47119140625\n",
      " disk-drive: 0.450927734375\n",
      " travel: 0.4091796875\n",
      "\n",
      "[query] mom\n",
      " tricky: 0.455322265625\n",
      " everything: 0.438232421875\n",
      " hud: 0.435302734375\n",
      " practical: 0.4287109375\n",
      " retailers: 0.427978515625\n"
     ]
    }
   ],
   "source": [
    "pkl_file = 'cbow_params.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f :\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params['word_vecs']\n",
    "    word_to_id = params['word_to_id']\n",
    "    id_to_word = params['id_to_word']\n",
    "\n",
    "querys = ['money', 'year', 'car', 'mom']\n",
    "for query in querys :\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d082f9-017e-43e1-b3f9-af3e50272a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d06652bb-108b-4c3c-bbe3-51c011da1ce9",
   "metadata": {},
   "source": [
    "## man - king + queen = women"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346c1bdd-493a-4c7f-8d09-534a12d3ffe0",
   "metadata": {},
   "source": [
    "### 단어벡터의 덧셈, 뺄셈으로 유추문제를 해결할 수 있다.\n",
    "### 단어의 의미뿐만이 아니라 문법적인 패턴도 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1ee23c7-63ef-4603-b093-1b40e843e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print('%s(을)를 찾을 수 없습니다.' % word)\n",
    "            return\n",
    "\n",
    "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4c582e1-0669-40c6-836f-3c45fb782f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] king:man = queen:?\n",
      " woman: 5.16015625\n",
      " veto: 4.9296875\n",
      " ounce: 4.69140625\n",
      " earthquake: 4.6328125\n",
      " successor: 4.609375\n"
     ]
    }
   ],
   "source": [
    "analogy('king', 'man', 'queen', word_to_id, id_to_word, word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cb08f2c-0f06-4af5-88d6-08928b08049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] take:took = buy:?\n",
      " shares: 5.5859375\n",
      " bought: 4.98828125\n",
      " owns: 4.6484375\n",
      " cents: 4.5703125\n",
      " chairman: 4.2265625\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(analogy('take', 'took', 'buy', word_to_id, id_to_word, word_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d02b4ad-cde7-4375-b794-4c3e6c2cb6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] take:took = eat:?\n",
      " could: 4.4453125\n",
      " might: 4.23046875\n",
      " street: 4.03125\n",
      " owns: 4.03125\n",
      " succeeds: 3.96875\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(analogy('take', 'took', 'eat', word_to_id, id_to_word, word_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d64b5ac7-0ec7-4771-ba56-47832045b8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] car:cars = person:?\n",
      " there: 4.93359375\n",
      " average: 4.78125\n",
      " you: 4.5234375\n",
      " yield: 4.51953125\n",
      " people: 4.45703125\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(analogy('car', 'cars', 'person', word_to_id, id_to_word, word_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7298290-b66d-47be-a6aa-b9fd848a9536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] people:person = year:?\n",
      " month: 9.2890625\n",
      " week: 8.0859375\n",
      " summer: 6.38671875\n",
      " article: 5.89453125\n",
      " quarter: 5.2421875\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(analogy('people', 'person', 'year', word_to_id, id_to_word, word_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7e7cfb5-9dae-4baa-8558-5c48ae8bd9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] body:shoulder = car:?\n",
      " share: 5.26171875\n",
      " fell: 5.15234375\n",
      " percentage: 4.1640625\n",
      " auto: 4.00390625\n",
      " hour: 3.982421875\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(analogy('body', 'shoulder', 'car', word_to_id, id_to_word, word_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e584bb38-4c21-4393-a213-786692d343c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] take:took = have:?\n",
      " has: 9.3828125\n",
      " had: 8.2890625\n",
      " 've: 6.99609375\n",
      " showed: 4.08984375\n",
      " owns: 3.97265625\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(analogy('take', 'took', 'have', word_to_id, id_to_word, word_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cbabf-57a8-48f6-be8b-619299c9b227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9f304-d505-45fa-9016-b2f0e8447d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
